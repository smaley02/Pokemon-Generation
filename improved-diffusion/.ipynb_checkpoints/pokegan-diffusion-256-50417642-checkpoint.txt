Wed Nov 20 12:13:06 EST 2024
c1107a-s35.ufhpc
/blue/rcstudents/smaley/pokegan/improved-diffusion
Logging to log_256
creating model and diffusion...
creating data loader...
training...
------------------------
| grad_norm | 3.08     |
| loss      | 1        |
| loss_q0   | 0.999    |
| loss_q1   | 1        |
| loss_q2   | 0.998    |
| loss_q3   | 1        |
| mse       | 1        |
| mse_q0    | 0.999    |
| mse_q1    | 1        |
| mse_q2    | 0.998    |
| mse_q3    | 1        |
| samples   | 16       |
| step      | 0        |
------------------------
saving model 0...
saving model 0.9999...
------------------------
| grad_norm | 3.24     |
| loss      | 0.904    |
| loss_q0   | 0.929    |
| loss_q1   | 0.896    |
| loss_q2   | 0.895    |
| loss_q3   | 0.892    |
| mse       | 0.904    |
| mse_q0    | 0.929    |
| mse_q1    | 0.896    |
| mse_q2    | 0.895    |
| mse_q3    | 0.892    |
| samples   | 176      |
| step      | 10       |
------------------------
------------------------
| grad_norm | 2.98     |
| loss      | 0.713    |
| loss_q0   | 0.747    |
| loss_q1   | 0.709    |
| loss_q2   | 0.703    |
| loss_q3   | 0.693    |
| mse       | 0.713    |
| mse_q0    | 0.747    |
| mse_q1    | 0.709    |
| mse_q2    | 0.703    |
| mse_q3    | 0.693    |
| samples   | 336      |
| step      | 20       |
------------------------
------------------------
| grad_norm | 2.59     |
| loss      | 0.534    |
| loss_q0   | 0.578    |
| loss_q1   | 0.519    |
| loss_q2   | 0.526    |
| loss_q3   | 0.518    |
| mse       | 0.534    |
| mse_q0    | 0.578    |
| mse_q1    | 0.519    |
| mse_q2    | 0.526    |
| mse_q3    | 0.518    |
| samples   | 496      |
| step      | 30       |
------------------------
------------------------
| grad_norm | 2.22     |
| loss      | 0.38     |
| loss_q0   | 0.415    |
| loss_q1   | 0.375    |
| loss_q2   | 0.358    |
| loss_q3   | 0.374    |
| mse       | 0.38     |
| mse_q0    | 0.415    |
| mse_q1    | 0.375    |
| mse_q2    | 0.358    |
| mse_q3    | 0.374    |
| samples   | 656      |
| step      | 40       |
------------------------
------------------------
| grad_norm | 1.91     |
| loss      | 0.273    |
| loss_q0   | 0.342    |
| loss_q1   | 0.253    |
| loss_q2   | 0.247    |
| loss_q3   | 0.251    |
| mse       | 0.273    |
| mse_q0    | 0.342    |
| mse_q1    | 0.253    |
| mse_q2    | 0.247    |
| mse_q3    | 0.251    |
| samples   | 816      |
| step      | 50       |
------------------------
------------------------
| grad_norm | 1.57     |
| loss      | 0.186    |
| loss_q0   | 0.253    |
| loss_q1   | 0.17     |
| loss_q2   | 0.167    |
| loss_q3   | 0.162    |
| mse       | 0.186    |
| mse_q0    | 0.253    |
| mse_q1    | 0.17     |
| mse_q2    | 0.167    |
| mse_q3    | 0.162    |
| samples   | 976      |
| step      | 60       |
------------------------
------------------------
| grad_norm | 1.22     |
| loss      | 0.123    |
| loss_q0   | 0.168    |
| loss_q1   | 0.113    |
| loss_q2   | 0.105    |
| loss_q3   | 0.102    |
| mse       | 0.123    |
| mse_q0    | 0.168    |
| mse_q1    | 0.113    |
| mse_q2    | 0.105    |
| mse_q3    | 0.102    |
| samples   | 1.14e+03 |
| step      | 70       |
------------------------
------------------------
| grad_norm | 1.14     |
| loss      | 0.0912   |
| loss_q0   | 0.146    |
| loss_q1   | 0.0727   |
| loss_q2   | 0.0657   |
| loss_q3   | 0.0618   |
| mse       | 0.0912   |
| mse_q0    | 0.146    |
| mse_q1    | 0.0727   |
| mse_q2    | 0.0657   |
| mse_q3    | 0.0618   |
| samples   | 1.3e+03  |
| step      | 80       |
------------------------
------------------------
| grad_norm | 0.718    |
| loss      | 0.0535   |
| loss_q0   | 0.0935   |
| loss_q1   | 0.0485   |
| loss_q2   | 0.0422   |
| loss_q3   | 0.0394   |
| mse       | 0.0535   |
| mse_q0    | 0.0935   |
| mse_q1    | 0.0485   |
| mse_q2    | 0.0422   |
| mse_q3    | 0.0394   |
| samples   | 1.46e+03 |
| step      | 90       |
------------------------
------------------------
| grad_norm | 0.59     |
| loss      | 0.0423   |
| loss_q0   | 0.0996   |
| loss_q1   | 0.0338   |
| loss_q2   | 0.026    |
| loss_q3   | 0.0242   |
| mse       | 0.0423   |
| mse_q0    | 0.0996   |
| mse_q1    | 0.0338   |
| mse_q2    | 0.026    |
| mse_q3    | 0.0242   |
| samples   | 1.62e+03 |
| step      | 100      |
------------------------
------------------------
| grad_norm | 0.675    |
| loss      | 0.054    |
| loss_q0   | 0.132    |
| loss_q1   | 0.0248   |
| loss_q2   | 0.0178   |
| loss_q3   | 0.0166   |
| mse       | 0.054    |
| mse_q0    | 0.132    |
| mse_q1    | 0.0248   |
| mse_q2    | 0.0178   |
| mse_q3    | 0.0166   |
| samples   | 1.78e+03 |
| step      | 110      |
------------------------
------------------------
| grad_norm | 0.545    |
| loss      | 0.0371   |
| loss_q0   | 0.0993   |
| loss_q1   | 0.0211   |
| loss_q2   | 0.0139   |
| loss_q3   | 0.0125   |
| mse       | 0.0371   |
| mse_q0    | 0.0993   |
| mse_q1    | 0.0211   |
| mse_q2    | 0.0139   |
| mse_q3    | 0.0125   |
| samples   | 1.94e+03 |
| step      | 120      |
------------------------
------------------------
| grad_norm | 0.423    |
| loss      | 0.0304   |
| loss_q0   | 0.085    |
| loss_q1   | 0.0221   |
| loss_q2   | 0.0122   |
| loss_q3   | 0.0103   |
| mse       | 0.0304   |
| mse_q0    | 0.085    |
| mse_q1    | 0.0221   |
| mse_q2    | 0.0122   |
| mse_q3    | 0.0103   |
| samples   | 2.1e+03  |
| step      | 130      |
------------------------
------------------------
| grad_norm | 0.417    |
| loss      | 0.0329   |
| loss_q0   | 0.0875   |
| loss_q1   | 0.0171   |
| loss_q2   | 0.00999  |
| loss_q3   | 0.00835  |
| mse       | 0.0329   |
| mse_q0    | 0.0875   |
| mse_q1    | 0.0171   |
| mse_q2    | 0.00999  |
| mse_q3    | 0.00835  |
| samples   | 2.26e+03 |
| step      | 140      |
------------------------
