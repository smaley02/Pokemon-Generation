{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokémon Name Generation with Keras\n",
    "\n",
    "Generate new unique Pokémon names with a LSTM using Andrej Karpathy's famous [Char-RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) which he used to generate poetry. There are more information in the blog, but the concept is fairly simple. We want the build a next-character-in-text predictor. We will do this by using a window of fixed length as our input and the next char as output and then train a LSTM to perform this task. Since the network won't understand raw characters we need to encode each character to a character vectors with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_length = 1    # The step length we take to get our samples from our corpus\n",
    "epochs = 100       # Number of times we train on our full data\n",
    "batch_size = 32    # Data samples in each training step\n",
    "latent_dim = 64    # Size of our LSTM\n",
    "dropout_rate = 0.2 # Regularization with dropout\n",
    "model_path = os.path.realpath('./poke_gen_model.h5') # Location for the model\n",
    "load_model = True # Enable loading model from disk\n",
    "store_model = True # Store model to disk after training\n",
    "verbosity = 1      # Print result for each epoch\n",
    "gen_amount = 1500    # How many "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "I have made a .txt where I have stored the names of Pokémon as rows. I have also done some ealy preprocessing like removing special characters and only using lowercase characters. To generate other things than Pokémon names the rows in this file can simply be replaced with some other text that one wishes to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Pokénames from file:\n",
      "bulbasaur\n",
      "chikorita\n",
      "treecko\n",
      "turtwig\n",
      "victini\n",
      "chespin\n",
      "rowlet\n",
      "ivysaur\n",
      "bayleef\n",
      "grovyle\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "input_path = os.path.realpath('./input/names.txt')\n",
    "\n",
    "input_names = []\n",
    "\n",
    "print('Reading Pokénames from file:')\n",
    "with open(input_path) as f:\n",
    "    for name in f:\n",
    "        name = name.rstrip()\n",
    "        if len(input_names) < 10:\n",
    "            print(name)\n",
    "        input_names.append(name)\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Concatenate all Pokémon names into a long string corpus.\n",
    "- Build dicionaries to translate chars to indices in a binary char vector.\n",
    "- Find a suitable sequence window, I base it on the longest name I find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 27\n",
      "Corpus length: 6734\n",
      "Number of names:  799\n",
      "Longest name:  12\n"
     ]
    }
   ],
   "source": [
    "# Make it all to a long string\n",
    "concat_names = '\\n'.join(input_names).lower()\n",
    "\n",
    "# Find all unique characters by using set()\n",
    "chars = sorted(list(set(concat_names)))\n",
    "num_chars = len(chars)\n",
    "\n",
    "# Build translation dictionaries, 'a' -> 0, 0 -> 'a'\n",
    "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
    "idx2char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Use longest name length as our sequence window\n",
    "max_sequence_length = max([len(name) for name in input_names])\n",
    "\n",
    "print('Total chars: {}'.format(num_chars))\n",
    "print('Corpus length:', len(concat_names))\n",
    "print('Number of names: ', len(input_names))\n",
    "print('Longest name: ', max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a training set where we take samples with sequence length as our input and the next char as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 6722\n",
      "First 10 sequences and next chars:\n",
      "X=[bulbasaur ch]   y=[i]\n",
      "X=[ulbasaur chi]   y=[k]\n",
      "X=[lbasaur chik]   y=[o]\n",
      "X=[basaur chiko]   y=[r]\n",
      "X=[asaur chikor]   y=[i]\n",
      "X=[saur chikori]   y=[t]\n",
      "X=[aur chikorit]   y=[a]\n",
      "X=[ur chikorita]   y=[ ]\n",
      "X=[r chikorita ]   y=[t]\n",
      "X=[ chikorita t]   y=[r]\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "# Loop over our data and extract pairs of sequances and next chars\n",
    "for i in range(0, len(concat_names) - max_sequence_length, step_length):\n",
    "    sequences.append(concat_names[i: i + max_sequence_length])\n",
    "    next_chars.append(concat_names[i + max_sequence_length])\n",
    "\n",
    "num_sequences = len(sequences)\n",
    "\n",
    "print('Number of sequences:', num_sequences)\n",
    "print('First 10 sequences and next chars:')\n",
    "for i in range(10):\n",
    "    print('X=[{}]   y=[{}]'.replace('\\n', ' ').format(sequences[i], next_chars[i]).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding our data into char vectors by using the translation dictionary from earlier.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- 'a'   => [1, 0, 0, ..., 0]\n",
    "\n",
    "- 'b'   => [0, 1, 0, ..., 0]\n",
    "\n",
    "- 'c'   => [0, 0, 1, ..., 0]\n",
    "\n",
    "- 'abc' => [[1, 0, 0, ..., 0], [0, 1, 0, ..., 0], [0, 0, 1, ..., 0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (6722, 12, 27)\n",
      "Y shape: (6722, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/51977825/ipykernel_1962678/2355450511.py:1: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.zeros((num_sequences, max_sequence_length, num_chars), dtype=np.bool)\n",
      "/scratch/local/51977825/ipykernel_1962678/2355450511.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = np.zeros((num_sequences, num_chars), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((num_sequences, max_sequence_length, num_chars), dtype=np.bool)\n",
    "Y = np.zeros((num_sequences, num_chars), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for j, char in enumerate(sequence):\n",
    "        X[i, j, char2idx[char]] = 1\n",
    "    Y[i, char2idx[next_chars[i]]] = 1\n",
    "    \n",
    "print('X shape: {}'.format(X.shape))\n",
    "print('Y shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "Build a standard LSTM network with: \n",
    "\n",
    "- Input shape: (max_sequence_length x num_chars) - representing our sequences.\n",
    "- Output shape: num_chars - representing the next char coming after each sequence.\n",
    "- Output activation: Softmax - since only one value should be 1 in output char vector.\n",
    "- Loss: Categorical cross-entrophy - standard loss for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                23552     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 27)                1755      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,307\n",
      "Trainable params: 25,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 11:10:15.297575: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 11:10:16.157544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78902 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "/apps/tensorflow/2.7.0/lib/python3.9/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(latent_dim, \n",
    "               input_shape=(max_sequence_length, num_chars),  \n",
    "               recurrent_dropout=dropout_rate))\n",
    "model.add(Dense(units=num_chars, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Watching the loss, doing cross-validation and all that good stuff is not that important here. The best model will not be found by optimizing some metric. We just want to strike a balance between a model that just output gibberish like 'sadsdaddddd' and model that memorizes the names it was trained on. For this it is better to just inspect the output and judge from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing model at: /blue/rcstudents/smaley/pokegan/name-generation/pokemon-name-generator/poke_gen_model.h5\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    model.load_weights(model_path)\n",
    "else:\n",
    "    \n",
    "    start = time.time()\n",
    "    print('Start training for {} epochs'.format(epochs))\n",
    "    history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=verbosity)\n",
    "    end = time.time()\n",
    "    print('Finished training - time elapsed:', (end - start)/60, 'min')\n",
    "    \n",
    "if store_model:\n",
    "    print('Storing model at:', model_path)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Generate names by starting with a real sequence from the corpus, continuously predicting the next char while updating the sequence. To get diversity the correct char is selected from a probability distribution based on the models prediction. This can also be furthered by something called temperature, which I didn't use here.\n",
    "\n",
    "I also added some postprocessing to remove things I did not like manually. Some of this could possibly be done by teaking the network, but I was happy with the way the names looked overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 new names are being generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 11:10:17.851964: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 150\n",
      "Generated 300\n",
      "Generated 450\n",
      "Generated 600\n",
      "Generated 750\n",
      "Generated 900\n",
      "Generated 1050\n",
      "Generated 1050\n",
      "Generated 1050\n",
      "Generated 1200\n",
      "Generated 1350\n",
      "Generated 1500\n"
     ]
    }
   ],
   "source": [
    "# Start sequence generation from end of the input sequence\n",
    "sequence = concat_names[-(max_sequence_length - 1):] + '\\n'\n",
    "\n",
    "new_names = []\n",
    "\n",
    "print('{} new names are being generated'.format(gen_amount))\n",
    "\n",
    "while len(new_names) < gen_amount:\n",
    "    \n",
    "    # Vectorize sequence for prediction\n",
    "    x = np.zeros((1, max_sequence_length, num_chars))\n",
    "    for i, char in enumerate(sequence):\n",
    "        x[0, i, char2idx[char]] = 1\n",
    "\n",
    "    # Sample next char from predicted probabilities\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    probs /= probs.sum()\n",
    "    next_idx = np.random.choice(len(probs), p=probs)   \n",
    "    next_char = idx2char[next_idx]   \n",
    "    sequence = sequence[1:] + next_char\n",
    "\n",
    "    # New line means we have a new name\n",
    "    if next_char == '\\n':\n",
    "\n",
    "        gen_name = [name for name in sequence.split('\\n')][1]\n",
    "\n",
    "        # Never start name with two identical chars, could probably also\n",
    "        if len(gen_name) > 2 and gen_name[0] == gen_name[1]:\n",
    "            gen_name = gen_name[1:]\n",
    "\n",
    "        # Discard all names that are too short\n",
    "        if len(gen_name) > 2:\n",
    "            \n",
    "            # Only allow new and unique names\n",
    "            if gen_name not in input_names + new_names:\n",
    "                new_names.append(gen_name.capitalize())\n",
    "\n",
    "        if 0 == (len(new_names) % (gen_amount/ 10)):\n",
    "            print('Generated {}'.format(len(new_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here are the results. I personally cannot tell the difference between generated names and names of Pokémon I dont know. Sometimes there are giveaways, but overall the names are convincing and diverse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 generated names:\n",
      "Forytrus\n",
      "Kraboys\n",
      "Ectove\n",
      "Panpow\n",
      "Onfisk\n",
      "Carchhoin\n",
      "Hichel\n",
      "Lugoan\n",
      "Blastorde\n",
      "Seedrold\n"
     ]
    }
   ],
   "source": [
    "print_first_n = min(10, gen_amount)\n",
    "\n",
    "print('First {} generated names:'.format(print_first_n))\n",
    "for name in new_names[:print_first_n]:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concat_output = '\\n'.join(sorted(new_names))\n",
    "output_path = os.path.realpath('./output/generated_names.txt')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "type_prediction",
   "language": "python",
   "name": "type_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
