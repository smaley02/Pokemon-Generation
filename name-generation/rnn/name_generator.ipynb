{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokémon Name Generation with Keras\n",
    "\n",
    "Generate new unique Pokémon names with a LSTM using Andrej Karpathy's famous [Char-RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) which he used to generate poetry. There are more information in the blog, but the concept is fairly simple. We want the build a next-character-in-text predictor. We will do this by using a window of fixed length as our input and the next char as output and then train a LSTM to perform this task. Since the network won't understand raw characters we need to encode each character to a character vectors with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_length = 1    # The step length we take to get our samples from our corpus\n",
    "epochs = 30       # Number of times we train on our full data\n",
    "batch_size = 32    # Data samples in each training step\n",
    "latent_dim = 64    # Size of our LSTM\n",
    "dropout_rate = 0.2 # Regularization with dropout\n",
    "model_path = os.path.realpath('./pokemon+5_perc_digimon.h5') # Location for the model\n",
    "load_model = False # Enable loading model from disk\n",
    "store_model = True # Store model to disk after training\n",
    "verbosity = 1      # Print result for each epoch\n",
    "gen_amount = 2000    # How many "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "I have made a .txt where I have stored the names of Pokémon as rows. I have also done some ealy preprocessing like removing special characters and only using lowercase characters. To generate other things than Pokémon names the rows in this file can simply be replaced with some other text that one wishes to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_path = os.path.realpath('../data/input/pokemon_and_five_perc_digimon.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Pokénames from file:\n",
      "corvisquire\n",
      "yanma\n",
      "zebstrika\n",
      "dunsparce\n",
      "grimmsnarl\n",
      "kangaskhan\n",
      "wigglytuff\n",
      "eldegoss\n",
      "hakamo\n",
      "fennekin\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "input_names = []\n",
    "\n",
    "print('Reading Pokénames from file:')\n",
    "with open(input_path) as f:\n",
    "    for name in f:\n",
    "        name = name.rstrip()\n",
    "        if len(input_names) < 10:\n",
    "            print(name)\n",
    "        input_names.append(name)\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Concatenate all Pokémon names into a long string corpus.\n",
    "- Build dicionaries to translate chars to indices in a binary char vector.\n",
    "- Find a suitable sequence window, I base it on the longest name I find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 30\n",
      "Corpus length: 9463\n",
      "Number of names:  1075\n",
      "Longest name:  28\n"
     ]
    }
   ],
   "source": [
    "# Make it all to a long string\n",
    "concat_names = '\\n'.join(input_names).lower()\n",
    "\n",
    "# Find all unique characters by using set()\n",
    "chars = sorted(list(set(concat_names)))\n",
    "num_chars = len(chars)\n",
    "\n",
    "# Build translation dictionaries, 'a' -> 0, 0 -> 'a'\n",
    "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
    "idx2char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Use longest name length as our sequence window\n",
    "max_sequence_length = max([len(name) for name in input_names])\n",
    "\n",
    "print('Total chars: {}'.format(num_chars))\n",
    "print('Corpus length:', len(concat_names))\n",
    "print('Number of names: ', len(input_names))\n",
    "print('Longest name: ', max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a training set where we take samples with sequence length as our input and the next char as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 9435\n",
      "First 10 sequences and next chars:\n",
      "X=[corvisquire yanma zebstrika ]   y=[d]\n",
      "X=[orvisquire yanma zebstrika d]   y=[u]\n",
      "X=[rvisquire yanma zebstrika du]   y=[n]\n",
      "X=[visquire yanma zebstrika dun]   y=[s]\n",
      "X=[isquire yanma zebstrika duns]   y=[p]\n",
      "X=[squire yanma zebstrika dunsp]   y=[a]\n",
      "X=[quire yanma zebstrika dunspa]   y=[r]\n",
      "X=[uire yanma zebstrika dunspar]   y=[c]\n",
      "X=[ire yanma zebstrika dunsparc]   y=[e]\n",
      "X=[re yanma zebstrika dunsparce]   y=[ ]\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "# Loop over our data and extract pairs of sequances and next chars\n",
    "for i in range(0, len(concat_names) - max_sequence_length, step_length):\n",
    "    sequences.append(concat_names[i: i + max_sequence_length])\n",
    "    next_chars.append(concat_names[i + max_sequence_length])\n",
    "\n",
    "num_sequences = len(sequences)\n",
    "\n",
    "print('Number of sequences:', num_sequences)\n",
    "print('First 10 sequences and next chars:')\n",
    "for i in range(10):\n",
    "    print('X=[{}]   y=[{}]'.replace('\\n', ' ').format(sequences[i], next_chars[i]).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding our data into char vectors by using the translation dictionary from earlier.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- 'a'   => [1, 0, 0, ..., 0]\n",
    "\n",
    "- 'b'   => [0, 1, 0, ..., 0]\n",
    "\n",
    "- 'c'   => [0, 0, 1, ..., 0]\n",
    "\n",
    "- 'abc' => [[1, 0, 0, ..., 0], [0, 1, 0, ..., 0], [0, 0, 1, ..., 0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (9435, 28, 30)\n",
      "Y shape: (9435, 30)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((num_sequences, max_sequence_length, num_chars), dtype=np.bool)\n",
    "Y = np.zeros((num_sequences, num_chars), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for j, char in enumerate(sequence):\n",
    "        X[i, j, char2idx[char]] = 1\n",
    "    Y[i, char2idx[next_chars[i]]] = 1\n",
    "    \n",
    "print('X shape: {}'.format(X.shape))\n",
    "print('Y shape: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "Build a standard LSTM network with: \n",
    "\n",
    "- Input shape: (max_sequence_length x num_chars) - representing our sequences.\n",
    "- Output shape: num_chars - representing the next char coming after each sequence.\n",
    "- Output activation: Softmax - since only one value should be 1 in output char vector.\n",
    "- Loss: Categorical cross-entrophy - standard loss for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 64)                24320     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                1950      \n",
      "=================================================================\n",
      "Total params: 26,270\n",
      "Trainable params: 26,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(latent_dim, \n",
    "               input_shape=(max_sequence_length, num_chars),  \n",
    "               recurrent_dropout=dropout_rate))\n",
    "model.add(Dense(units=num_chars, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Watching the loss, doing cross-validation and all that good stuff is not that important here. The best model will not be found by optimizing some metric. We just want to strike a balance between a model that just output gibberish like 'sadsdaddddd' and model that memorizes the names it was trained on. For this it is better to just inspect the output and judge from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 30 epochs\n",
      "Epoch 1/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.7688\n",
      "Epoch 2/30\n",
      "295/295 [==============================] - 13s 44ms/step - loss: 2.5389\n",
      "Epoch 3/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.4641\n",
      "Epoch 4/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.4018\n",
      "Epoch 5/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.3439\n",
      "Epoch 6/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.2977\n",
      "Epoch 7/30\n",
      "295/295 [==============================] - 13s 44ms/step - loss: 2.2542\n",
      "Epoch 8/30\n",
      "295/295 [==============================] - 13s 45ms/step - loss: 2.2174\n",
      "Epoch 9/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.1835\n",
      "Epoch 10/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.1475\n",
      "Epoch 11/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.1111\n",
      "Epoch 12/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.0959\n",
      "Epoch 13/30\n",
      "295/295 [==============================] - 13s 45ms/step - loss: 2.0739\n",
      "Epoch 14/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.0651\n",
      "Epoch 15/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.0258\n",
      "Epoch 16/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.0202\n",
      "Epoch 17/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 2.0019\n",
      "Epoch 18/30\n",
      "295/295 [==============================] - 13s 45ms/step - loss: 1.9870\n",
      "Epoch 19/30\n",
      "295/295 [==============================] - 13s 46ms/step - loss: 1.9732\n",
      "Epoch 20/30\n",
      "295/295 [==============================] - 13s 44ms/step - loss: 1.9630\n",
      "Epoch 21/30\n",
      "295/295 [==============================] - 13s 45ms/step - loss: 1.9545\n",
      "Epoch 22/30\n",
      "295/295 [==============================] - 13s 44ms/step - loss: 1.9375\n",
      "Epoch 23/30\n",
      "295/295 [==============================] - 13s 44ms/step - loss: 1.9295\n",
      "Epoch 24/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 1.9103\n",
      "Epoch 25/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 1.9113\n",
      "Epoch 26/30\n",
      "295/295 [==============================] - 13s 44ms/step - loss: 1.8999\n",
      "Epoch 27/30\n",
      "295/295 [==============================] - 13s 44ms/step - loss: 1.8862\n",
      "Epoch 28/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 1.8768\n",
      "Epoch 29/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 1.8775\n",
      "Epoch 30/30\n",
      "295/295 [==============================] - 13s 43ms/step - loss: 1.8645\n",
      "Finished training - time elapsed: 6.506796908378601 min\n",
      "Storing model at: /blue/rcstudents/smaley/pokegan/name-generation/rnn/pokemon+5_perc_digimon.h5\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    model.load_weights(model_path)\n",
    "else:\n",
    "    \n",
    "    start = time.time()\n",
    "    print('Start training for {} epochs'.format(epochs))\n",
    "    history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=verbosity)\n",
    "    end = time.time()\n",
    "    print('Finished training - time elapsed:', (end - start)/60, 'min')\n",
    "    \n",
    "if store_model:\n",
    "    print('Storing model at:', model_path)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Generate names by starting with a real sequence from the corpus, continuously predicting the next char while updating the sequence. To get diversity the correct char is selected from a probability distribution based on the models prediction. This can also be furthered by something called temperature, which I didn't use here.\n",
    "\n",
    "I also added some postprocessing to remove things I did not like manually. Some of this could possibly be done by teaking the network, but I was happy with the way the names looked overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 new names are being generated\n",
      "Generated 0\n",
      "Generated 200\n",
      "Generated 400\n",
      "Generated 600\n",
      "Generated 800\n",
      "Generated 1000\n",
      "Generated 1200\n",
      "Generated 1400\n",
      "Generated 1600\n",
      "Generated 1800\n",
      "Generated 2000\n"
     ]
    }
   ],
   "source": [
    "# Start sequence generation from end of the input sequence\n",
    "sequence = concat_names[-(max_sequence_length - 1):] + '\\n'\n",
    "\n",
    "new_names = []\n",
    "\n",
    "print('{} new names are being generated'.format(gen_amount))\n",
    "\n",
    "while len(new_names) < gen_amount:\n",
    "    \n",
    "    # Vectorize sequence for prediction\n",
    "    x = np.zeros((1, max_sequence_length, num_chars))\n",
    "    for i, char in enumerate(sequence):\n",
    "        x[0, i, char2idx[char]] = 1\n",
    "\n",
    "    # Sample next char from predicted probabilities\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    probs /= probs.sum()\n",
    "    next_idx = np.random.choice(len(probs), p=probs)   \n",
    "    next_char = idx2char[next_idx]   \n",
    "    sequence = sequence[1:] + next_char\n",
    "\n",
    "    # New line means we have a new name\n",
    "    if next_char == '\\n':\n",
    "\n",
    "        gen_name = [name for name in sequence.split('\\n')][1]\n",
    "\n",
    "        # Never start name with two identical chars, could probably also\n",
    "        if len(gen_name) > 2 and gen_name[0] == gen_name[1]:\n",
    "            gen_name = gen_name[1:]\n",
    "\n",
    "        # Discard all names that are too short\n",
    "        if len(gen_name) > 2:\n",
    "            \n",
    "            # Only allow new and unique names\n",
    "            if gen_name not in input_names + new_names:\n",
    "                new_names.append(gen_name.capitalize())\n",
    "\n",
    "        if 0 == (len(new_names) % (gen_amount/ 10)):\n",
    "            print('Generated {}'.format(len(new_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here are the results. I personally cannot tell the difference between generated names and names of Pokémon I dont know. Sometimes there are giveaways, but overall the names are convincing and diverse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 generated names:\n",
      "Gothimon thakegom\n",
      "Fwissect\n",
      "Flubf\n",
      "Ealexgurdramon\n",
      "Amoleds\n",
      "Amoleds\n",
      "Zoloth\n",
      "Barre\n",
      "Murmagmite\n",
      "Armanders\n",
      "Houndon\n",
      "Houndon\n",
      "Miscubitu\n",
      "Erhipy\n",
      "Nyree\n",
      "Nator\n",
      "Nickeavole\n",
      "Nickeavole\n",
      "Heroar\n",
      "Madwool\n",
      "Cregishoass\n",
      "Sepede\n",
      "Klaink\n",
      "Chyril\n",
      "Cuntolipe\n",
      "Slabbilet\n",
      "Limpla\n",
      "Goltat\n",
      "Olaking\n",
      "Audoris\n",
      "Floawgaede\n",
      "Dermie\n",
      "Elermarp\n",
      "Mome\n",
      "Dramplan\n",
      "Capbumb\n",
      "Belessiker\n",
      "Belessiker\n",
      "Xradiois\n",
      "Milpy\n",
      "Meltom\n",
      "Vibwion\n",
      "Teragee\n",
      "Stazil\n",
      "Ncrowatt\n",
      "Cawnilut\n",
      "Sping\n",
      "Bellipede\n",
      "Sandoous\n",
      "Regattasic\n",
      "Regattasic\n",
      "Tototau\n",
      "Raperat\n",
      "Valolty\n",
      "Skoopuff\n",
      "Flacels\n",
      "Wughla\n",
      "Bramble\n",
      "Rokorita\n",
      "Amproke\n",
      "Argowloot\n",
      "Chiboude\n",
      "Narudon\n",
      "Droduik\n",
      "Dearemibolocr\n",
      "Kickitu\n",
      "Kickitu\n",
      "Bentimon\n",
      "Darchic\n",
      "Flaucobwa\n",
      "Mawnklett\n",
      "Arditurp\n",
      "Arditurp\n",
      "Niilorr\n",
      "Consurrus\n",
      "Consurrus\n",
      "Blidoedre\n",
      "Hatmo\n",
      "Elucherna\n",
      "Elucherna\n",
      "Angginiar\n",
      "Grubin\n",
      "Mefselis\n",
      "Dealish\n",
      "Torvizar\n",
      "Imime\n",
      "Panster\n",
      "Ropdee\n",
      "Mulphor\n",
      "Magemama\n",
      "Skantrit\n",
      "Backruu\n",
      "Gleampla\n",
      "Krobuble\n",
      "Sirphenh\n",
      "Dusteen\n",
      "Retinat\n",
      "Trounchen\n",
      "Ballox\n",
      "Mriarcion\n"
     ]
    }
   ],
   "source": [
    "print_first_n = min(100, gen_amount)\n",
    "\n",
    "print('First {} generated names:'.format(print_first_n))\n",
    "for name in new_names[:print_first_n]:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concat_output = '\\n'.join(sorted(new_names))\n",
    "output_path = os.path.realpath('./output/pokemon+5_perc_digimon_2000_generated_names.txt')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "type_prediction",
   "language": "python",
   "name": "type_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
